import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import torch
import json
import requests

os.environ['HF_HOME'] = 'D:/my_models_cache'

# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨ï¼Œå¹¶è®¾ç½®è®¡ç®—è®¾å¤‡
if torch.cuda.is_available():
    device = "cuda"
    print("CUDA is available! Using GPU for computation.")
else:
    device = "cpu"
    print("CUDA is NOT available. Using CPU for computation.")

from dotenv import load_dotenv
load_dotenv()
from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader , Docx2txtLoader , DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI

# --- é…ç½® ---
FOLDER_PATH = "E:/self-cultivation/2025å°å­¦æœŸ/ä½œä¸š/å¤§å®éªŒ/sx/æ•°æ®"
EMBEDDING_MODEL = "BAAI/bge-small-zh-v1.5"
CHUNK_SIZE = 600
CHUNK_OVERLAP = 120
FAISS_DB_PATH = "./faiss_index"
METADATA_FILE_NAME = "documents_metadata.json"

# --- æ–°å¢ SiliconFlow Reranker é…ç½® ---
SILICONFLOW_API_KEY =  os.getenv("SILICONFLOW_API_KEY")
SILICONFLOW_RERANK_URL = "https://api.siliconflow.cn/v1/rerank"
RERANKER_MODEL = "BAAI/bge-reranker-v2-m3" # ç”¨æˆ·æä¾›çš„æ¨¡å‹åç§°

API_KEY = "sk-uyysjarbjuecjrgxacqnbbdrulbjlrtlsymfnkazrfinilee"

# 1. åŠ è½½æ–‡ä»¶å¤¹ä¸­çš„æ•°æ®
def load_documents_from_folder(folder_path:str):
    documents = []

    if not os.path.exists(folder_path):
        raise FileNotFoundError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼š{folder_path}")

    for file in os.listdir(folder_path):
        file_path = os.path.join(folder_path,file)
        try:
            if file.lower().endswith('.pdf'):
                # åŠ è½½PDFæ–‡ä»¶
                loader = PyPDFLoader(file_path)
                documents.extend(loader.load())
                print(f"âœ“ å·²åŠ è½½PDFæ–‡ä»¶: {file}")

            elif file.lower().endswith('.docx'):
                # åŠ è½½DOCXæ–‡ä»¶
                loader = Docx2txtLoader(file_path)
                documents.extend(loader.load())
                print(f"âœ“ å·²åŠ è½½DOCXæ–‡ä»¶: {file}")

        except Exception as e:
            print(f"Ã— åŠ è½½å¤±è´¥ {file}: {str(e)}")
            continue
    print("ğŸ“ æ–‡æ¡£åŠ è½½æˆåŠŸï¼")
    return documents

# 2. å¯¹æ–‡æ¡£è¿›è¡Œåˆ‡å—
def split_documents(documents):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size = CHUNK_SIZE,
        chunk_overlap = CHUNK_OVERLAP,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?"],
        length_function = len,
        add_start_index=True,
    )
    chunks = text_splitter.split_documents(documents)
    print(f"âœ‚ï¸ åˆ‡åˆ†åå¾—åˆ° {len(chunks)} ä¸ªæ–‡æœ¬å—")
    return chunks

# 3. åŠ è½½åµŒå…¥æ¨¡å‹
def get_embeddings_model(model_name_or_path: str) -> HuggingFaceEmbeddings:
    print(f"\næ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {model_name_or_path} (device: {device})")
    # è¿™é‡Œç¡®ä¿æ¨¡å‹ä¼šä¸‹è½½åˆ°HF_HOMEæŒ‡å®šç›®å½•ï¼Œå¹¶ä½¿ç”¨æŒ‡å®šè®¾å¤‡
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name_or_path,
        model_kwargs={'device': device}
    )
    print("âœ“ åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆã€‚")
    return embeddings

# 4. åˆ›å»º FAISS å‘é‡æ•°æ®åº“å¹¶ä¿å­˜åˆ°æœ¬åœ°
def create_and_save_faiss_db(chunks: list[Document], embeddings_model: HuggingFaceEmbeddings, db_path: str) -> FAISS:
    print("\næ­£åœ¨åˆ›å»º FAISS å‘é‡æ•°æ®åº“...")
    faiss_db = FAISS.from_documents(chunks, embeddings_model)
    print(f"âœ“ FAISS å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆã€‚æ­£åœ¨ä¿å­˜åˆ°: {db_path}")
    faiss_db.save_local(db_path)
    print("ğŸ”§ FAISS å‘é‡æ•°æ®åº“ä¿å­˜æˆåŠŸã€‚")
    return faiss_db

# 5. åˆ›å»ºå¹¶ä¿å­˜æ–‡æ¡£å—çš„å…ƒæ•°æ®
def create_and_save_metadata(chunks: list[Document], output_dir: str, metadata_file_name: str):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    metadata_list = []
    for i, chunk in enumerate(chunks):
        metadata = {
            "chunk_id": f"chunk_{i}",
            "page_content_preview": chunk.page_content[:200] + "...", # é¢„è§ˆå‰200å­—ç¬¦
            "source": chunk.metadata.get('source', 'æœªçŸ¥æ–‡ä»¶'),
            "page": chunk.metadata.get('page', 'æœªçŸ¥é¡µç '),
            "start_index": chunk.metadata.get('start_index', 'æœªçŸ¥ç´¢å¼•')
        }
        metadata_list.append(metadata)

    metadata_file_path = os.path.join(output_dir, metadata_file_name)
    with open(metadata_file_path, 'w', encoding='utf-8') as f:
        json.dump(metadata_list, f, ensure_ascii=False, indent=4)
    print(f"\nå…ƒæ•°æ®å·²ä¿å­˜åˆ°ï¼š{metadata_file_path}")

# --- SiliconFlow Reranker å‡½æ•° ---
def rerank_documents_siliconflow(
    query: str,
    documents: list[Document], # æ¥æ”¶ LangChain Document å¯¹è±¡åˆ—è¡¨
    model: str = RERANKER_MODEL,
    api_key: str = SILICONFLOW_API_KEY,
    top_n: int = 5 # é‡æ’åè¿”å›çš„é¡¶éƒ¨æ–‡æ¡£æ•°é‡
) -> list[Document]:
    # ä½¿ç”¨ SiliconFlow Rerank API å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’
    if not api_key:
        print("é”™è¯¯ï¼šæœªè®¾ç½® SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡ï¼Œæ— æ³•è°ƒç”¨ Reranker APIã€‚")
        return []

    doc_contents = [doc.page_content for doc in documents] # æå–æ–‡æ¡£å†…å®¹å‘é€ç»™API

    payload = {
        "model": model,
        "query": query,
        "documents": doc_contents
    }
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    print(f"\næ­£åœ¨è°ƒç”¨ SiliconFlow Reranker ({model})...")
    try:
        response = requests.post(SILICONFLOW_RERANK_URL, json=payload, headers=headers, timeout=30)
        response.raise_for_status() # å¯¹ 4xx æˆ– 5xx HTTP çŠ¶æ€ç æŠ›å‡ºå¼‚å¸¸
        result = response.json()

        if "results" not in result:
            print(f"Reranker API å“åº”æ ¼å¼ä¸æ­£ç¡®: {result}")
            return []

        # å°† rerank ç»“æœä¸åŸå§‹ Document å¯¹è±¡å…³è”å¹¶æ’åº
        reranked_items = []
        # SiliconFlow API çš„ results æ•°ç»„é¡ºåºä¸ä¼ å…¥çš„ documents æ•°ç»„é¡ºåºä¸€è‡´
        for i, res in enumerate(result["results"]):
            # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹ Document å¯¹è±¡ã€‚è¿™é‡Œå‡è®¾ res['document'] ç›´æ¥æ˜¯åŸå§‹å†…å®¹
            # æ›´ä¸¥è°¨çš„è¯ï¼Œå¯ä»¥åœ¨ä¼ å…¥ documents æ—¶ç»™ Document å¯¹è±¡æ·»åŠ å”¯ä¸€ IDï¼Œç„¶åé€šè¿‡ ID åŒ¹é…
            # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å‡è®¾ res['document'] ä¸ doc_contents[i] ç›¸åŒ
            reranked_items.append({
                "document": documents[i], # åŸå§‹ LangChain Document å¯¹è±¡
                "score": res['relevance_score']
            })

        # æŒ‰ç›¸å…³æ€§åˆ†æ•°é™åºæ’åº
        reranked_items.sort(key=lambda x: x['score'], reverse=True)

        # æå– top_n æ–‡æ¡£
        top_reranked_docs = [item['document'] for item in reranked_items[:top_n]]

        # print(f"\nRerank æˆåŠŸï¼Œè¿”å› Top {len(top_reranked_docs)} æ–‡æ¡£ã€‚\n")
        # for i, doc_item in enumerate(reranked_items[:top_n]):
        #     print(f"Top {i+1} (Score: {doc_item['score']:.4f}): {doc_item['document'].page_content[:100]}...")

        return top_reranked_docs

    except requests.exceptions.RequestException as e:
        print(f"è°ƒç”¨ SiliconFlow Reranker API å¤±è´¥: ç½‘ç»œæˆ–è¯·æ±‚é”™è¯¯ - {e}")
        return []
    except json.JSONDecodeError:
        print(f"Reranker API å“åº”ä¸æ˜¯æœ‰æ•ˆçš„ JSON: {response.text}")
        return []
    except Exception as e:
        print(f"Reranker å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return []

# 6. LLM ç”Ÿæˆç­”æ¡ˆ
def generate_llm_response(query: str, top_documents: list[Document]) -> str:
    """
        ä½¿ç”¨ LLM å¯¹ top N æ–‡æ¡£å†…å®¹è¿›è¡Œæ€»ç»“ã€åˆå¹¶ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–ç­”æ¡ˆ
    """
    context_text = "\n\n".join(
        f"æ®µè½{i + 1}ï¼š{doc.page_content}" for i, doc in enumerate(top_documents)
    )
    prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªç²¾é€šä¸­å›½æ³•å¾‹çš„AIåŠ©æ‰‹ï¼Œç°åœ¨æœ‰ç”¨æˆ·å‘ä½ æå‡ºæ³•å¾‹é—®é¢˜ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹è¿›è¡Œæ€»ç»“ã€å½’çº³ï¼Œè¾“å‡ºæƒå¨ä¸”ç®€æ´çš„æ³•å¾‹åˆ†æç»“æœã€‚
    
        ç”¨æˆ·é—®é¢˜ï¼š
        {query}
    
        ç›¸å…³å‚è€ƒæ–‡æ¡£ï¼š
        {context_text}
    
        è¯·æ ¹æ®ä¸Šè¿°å†…å®¹ç”Ÿæˆä¸€æ®µæ³•å¾‹åˆ†æè¯´æ˜ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š
        1. è¯­è¨€å®¢è§‚ã€ä¸“ä¸šã€æ¸…æ™°ï¼›
        2. ä¸è¦å¼•ç”¨åŸæ–‡æ®µè½ç¼–å·ï¼Œæ•´åˆæˆæµç•…æ–‡æœ¬ï¼›
        3. è‹¥æœ‰å¤šä¸ªæ–¹é¢ï¼Œè¯·ç”¨è¦ç‚¹åˆ—ä¸¾ï¼ˆå¦‚ "â‘ ...â‘¡..."ï¼‰ï¼›
        4. è‹¥æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·è¯´æ˜â€œæ— æ³•å‡†ç¡®åˆ¤æ–­â€ã€‚
    
        æœ€ç»ˆè¾“å‡ºï¼š
    """
    try:
        llm = ChatOpenAI(
            model="Qwen/QwQ-32B",
            api_key= API_KEY,
            base_url="https://api.siliconflow.cn/v1",
            temperature=0.3
        )

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç²¾é€šä¸­å›½æ³•å¾‹çš„æ³•å¾‹åŠ©æ‰‹ï¼Œå–„äºå½’çº³æ€»ç»“å¹¶è§£é‡Šæ³•å¾‹é—®é¢˜ã€‚"},
            {"role": "user", "content": prompt}
        ]

        response = llm.invoke(messages)

        # æ ¹æ®å®é™…è¿”å›ç»“æ„åˆ¤æ–­æ˜¯ content å­—æ®µè¿˜æ˜¯ json æ ¼å¼
        try:
            result = json.loads(response.content)
            return result.get("output", str(result))  # æ”¯æŒå¤šç§æ ¼å¼
        except Exception:
            return response.content if isinstance(response.content, str) else str(response.content)

    except Exception as e:
        print(f"âš ï¸ LLM æ€»ç»“å¤±è´¥ï¼š{e}")
        return "âŒ æ— æ³•ç”Ÿæˆæ³•å¾‹åˆ†æï¼Œè¯·æ£€æŸ¥APIå¯†é’¥æˆ–ç½‘ç»œè¿æ¥ã€‚"

if __name__ == "__main__":
    # 1. åŠ è½½æ–‡æ¡£
    docs = load_documents_from_folder(FOLDER_PATH)
    print(f"ğŸ“‘ å…±åŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£é¡µ")

    # 2. æ–‡æœ¬åˆ‡å—
    chunks_docs = split_documents(docs)

    # 3. åˆå§‹åŒ–åµŒå…¥å‘é‡
    embeddings = get_embeddings_model(EMBEDDING_MODEL)

    # 4. åˆ›å»ºå¹¶ä¿å­˜ FAISS å‘é‡æ•°æ®åº“
    faiss_db = create_and_save_faiss_db(chunks_docs, embeddings, FAISS_DB_PATH)

    # 5. åˆ›å»ºå¹¶ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶
    create_and_save_metadata(chunks_docs,FAISS_DB_PATH,METADATA_FILE_NAME)

    print("\n âœ… è¿›ç¨‹æˆåŠŸå®Œæˆï¼ ")
    print(f"ğŸ“Œ FAISS ç´¢å¼•å’Œå…ƒæ•°æ®å­˜å‚¨åœ¨ï¼š{os.path.abspath(FAISS_DB_PATH)}")

    print("\n --- Rerank åŠŸèƒ½ç¤ºä¾‹ ---")
    loaded_model = get_embeddings_model(EMBEDDING_MODEL)
    loaded_faiss_db = FAISS.load_local(FAISS_DB_PATH, loaded_model, allow_dangerous_deserialization=True)
    print(f"ğŸ” å·²ä» {FAISS_DB_PATH} åŠ è½½ FAISS æ•°æ®åº“ã€‚")

    while True:
        user_query = input("\nğŸ’¬ è¯·è¾“å…¥ä½ çš„æ³•å¾‹é—®é¢˜ï¼ˆè¾“å…¥ q é€€å‡ºï¼‰ï¼š\n>>> ").strip()
        if user_query.lower() in ["q", "quit", "exit"]:
            print("ğŸ‘‹ å·²é€€å‡ºã€‚")
            break
        if not user_query:
            print("âš ï¸ è¾“å…¥ä¸èƒ½ä¸ºç©ºã€‚")
            continue

        retriever = loaded_faiss_db.as_retriever(search_kwargs={"k": 8})
        initial_retrieved_docs = retriever.invoke(user_query)
        # print(f"\n--- ğŸ” åˆå§‹æ£€ç´¢åˆ° {len(initial_retrieved_docs)} ç¯‡æ–‡æ¡£ ---")
        # for i, doc in enumerate(initial_retrieved_docs):
        #     print(f" åˆå§‹ Top {i + 1}: {doc.page_content[:100]}...")

        top_reranked_docs = rerank_documents_siliconflow(user_query, initial_retrieved_docs, top_n=5)

        print("\nğŸš€ æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")
        final_answer = generate_llm_response(user_query, top_reranked_docs)
        print(f"\nğŸ“¢ å›ç­”ï¼š\n{final_answer}")



    # è°ƒç”¨ Reranker å¯¹åˆæ­¥æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œé‡æ’
    # top_reranked_docs = rerank_documents_siliconflow(user_query, initial_retrieved_docs, top_n=3)
    # if top_reranked_docs:
    #     print("\n--- ğŸ“š é‡æ’åçš„æ–‡æ¡£ï¼ˆç”¨äºæœ€ç»ˆç­”æ¡ˆç”Ÿæˆï¼‰---")
    #     for i, doc in enumerate(top_reranked_docs):
    #         print(f"æœ€ç»ˆ Top {i + 1}: {doc.page_content[:200]}...")
    #
    #     print("\nğŸš€ æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")
    #     final_answer = generate_llm_response(user_query, top_reranked_docs)
    #     print(f"\nğŸ’¬ é—®é¢˜ï¼š{user_query}")
    #     print("ğŸ“¢ LLM ç”Ÿæˆçš„æ€»ç»“å›ç­”ï¼š")
    #     print("-" * 60)
    #     print(final_answer)
    #     print("-" * 60)
    #
    # else:
    #     print("\næœªèƒ½æˆåŠŸé‡æ’æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ API å¯†é’¥å’Œç½‘ç»œè¿æ¥ã€‚")